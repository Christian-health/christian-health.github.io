## 一、服务器性能问题的引入

​    假设我们使用`LAMP`搭建了一个网站，并发状态的请求有`200`个,而其中的 `50`个请求是 `dynamic（动态的）`那么我们估算它需要多少资源，假设我们的`apache`工作在`prefork`模式下，大概每一个进程所占用的内存空间应该有`2M`，涉及到的动态进程，一个动态进程内存空间大概是`10M`左右。`500M+150*2M`所以大致需要`800M`的内存，这`50`个动态进程，每个在执行的时候可能和本地的`mysql`进行交互，`mysql`是`cup`密集型的操作，而且进程切换本身也需要`cpu`，所以我们配置了一个2个`cpu`和`4g`内存，另外假设`200`个请求同时到达，那么我们的网卡要开始进行处理，还是需要`cpu` 。因为到达网卡上的`http`请求，是通过硬件网卡上来的，所以首先要到达内核空间，但是我们的`web`服务器是运行在用户空间的，所以需要内核空间进行一定的处理传递给用户空间的`web`服务器。

   因为有`200`个并发请求，那么意味着随时随地可能有`200`个`IO`。而几乎每一个`IO`都需要有`CPU`的参与来执行的。而且每个用户的请求数据都位于磁盘上。磁盘上的数据只有被操作系统内核载入到内核空间才能通过`web`服务器经过网卡发送给用户，所以`web`请求的背后，会涉及到大量的网络`IO`和磁盘`IO`。而这些不可避免的会影响我们网络服务器的性能的。假如网站越来越受到环境，并发请求变成了`1000`，那么其中有动态请求是`200`。什么是动态请求，什么是静态请求，比如请求`html，css，img`等等这些都是静态请求，需要服务器程序运行的就是动态请求。动态请求的执行速度很慢的，比静态请求慢。那么这个时候需要的资源就是`800*2+200*10`，那么这个时候资源开始吃紧了。我们还是两个`cpu`但是我们有`1000`个进程，所以一个`cpu`上会有500个进程进行切换，那么这个时候光进行切换，就需要很多的资源，所以我们的服务器应该相当的吃力了。

​    还有假如我们的带宽上线只有`400`个进程，那么另外的`600`个都需要进行排队。这个时候用户体验就会非常的差。研究发现一个网站打开时间超过`3s`就会流失`60%`的用户。



## 二、解决方法

（1）`Scala on向上扩展` 

​     就是换服务器，原来是`4g`内存，2个`cpu`，那么我们换成`16g`内存，`8`个`cpu`。所以通过调换一台有这更强处理能力的服务器，这个就是`Scala on`。但是一般来说尽管变成了`16g`内存和`8`个`cpu`。但是我们带来的性能并不是原来的`4`倍。这个不是线性的，而是类似于sin(x)的走到顶峰就会下降。`8`个cpu会产生争夺，需要仲裁。仲裁本身还需要资源，这个需要的资源代价也很大。另外硬件的价格非常的贵。而且当请求变成`10万`，你能找到这样的服务器吗？

（2）`Scala out向外扩展`

​    一台服务器如果能处理`200`那么我就多加上几台，这样不就能处理`1000`了吗？但是问题来了，我们怎么能将用户的请求分到三台不同的服务器上。所以引入了负载均衡！

比如使用`DNS`的`A`记录（见参考文献1）有如下：

```shell
www.baidu.com.cn     IN     A   172.100.120.1
www.baidu.com.cn     IN     A   172.100.120.2
www.baidu.com.cn     IN     A   172.100.120.3
```

​    那么不同的用户请求来了，`DNS`会返回不同的`IP`所以这样就可以访问不同的服务器地址了。但是我们不能简单的使用`DNS`来进行负载均衡。为什么，因为`DNS`是有缓存的！这样就会导致用户始终访问一个服务器，就起不到负载均衡的效果了。

   怎么能让用户的请求，公平的平均的分配到各个服务器？所以我们需要一个在服务器前面放一个特殊设备，当请求来了，到达这个特殊设备，由这个特殊设备来把请求分配给后面的服务器。最简单的调度算法就是轮训（`RR`）。

   一个浏览器可以并发的起多个`http`请求，比如`chrome`应该是`6`个假设这`6`个请求分配到不同的服务器上了，那么就同时进行了请求，那么速度就非常的快。所以我们在浏览器中请求的对象可能来自多台不同的服务器。

调度算法有：

```
RR   轮训
WRR  加权轮训
```

![高可用-最初架构](https://github.com/Christian-health/christian-health.github.io/blob/master/img/%E9%AB%98%E5%8F%AF%E7%94%A8-%E6%9C%80%E5%88%9D%E6%9E%B6%E6%9E%84.jpg?raw=true)

​     架构的最前面防止一个负载均衡器，将到来的`http`请求分派给不同的`webserver`，然后为了避免一个用户在第一个服务器上写了一个文章，然后再次连接的时候，它的请求被分配到第二个服务器上了，就看不到刚刚写的文章了，那么为了解决这个问题，就需要让三个服务器访问同样的一个数据库，所以在`webserver`后面添加了个数据库。但是图片是不能存放到数据库中的，数据库不能存放图片，那么这个图片放在硬盘上的一个目录里面，那么把这个文件的连接存放到数据库里面，这样就实现了附件图片的上传。所以又需要让三个服务器访问同样的一个存放文件的硬盘空间。所以添加了一个`NFS`也就是网络文件系统。这样三个服务器都可以访问到图片了。

​     页面文件`html`和`css`存放在哪里？直接放在服务器本地，三个服务器都放置上。但是如果过几天服务器上的静态页面需要变更添加新功能，只有三台还好弄，但是如果是`30`台，怎么办？难道都要手动的换？所以需要使用`rsync`就是一个文件的同步复制工具。`rsync`是一个高效的，远程复制工具，能实现增量复制。那么怎么及时的通知其他的服务器来同步数据那 ？我们有`rsync+inotify`（见参考文献2）。`rsync`的读音是`r,si:nk`。 文件内容发生改变是内核空间监控的，只有内核空间把变更发给用户空间，用户才能知道文件发生了改变。

​    还有工作在被动模式下的`ftp`，前端的每一个大于`1024`端口的请求都要代理到后端主机上去。还有两台`mysql`如何进行更新。

​    还有`loadbalance`，如果我们的网站的规模越来越大，那么我们的`loadbalance`的性能早晚会达到上限，那么怎么办？这个时候不能采用在前面添加多个`loadbalance`的方法，因为多个`loadbalance`前面加上一个`loadbalance`又形成了新的性能瓶颈。所以这个时候要使用其他的方法解决这个问题，这个解决方法就是：功能切分。把一个大的功能切分成小的功能，每一个小的功能做成一个集群。比如一个`www.sina.com`我在这个页面上点击新闻，然后发现跳转到`www.news.sina.com`这个网站上面了。这就是把一个大的页面的功能拆分了。所以我点击新浪上的某一个标签，也就是某一个功能，请求就可能被分发到另外的一个集群上，每一个小的功能，都在一个集群上。看他们所有标签在同一个主页上，但是背后的服务器集群确是不同的。

![HA集群](https://github.com/Christian-health/christian-health.github.io/blob/master/img/HA%E9%9B%86%E7%BE%A4.jpg?raw=true)

   如上图，如果调度器`loadbalance`挂掉了，怎么办？所以我们需要一个`standby（备用的）`调度器。主备之间有一个`heartbeat`心跳，如果备份节点发现心跳断掉了，那么就启动。如果某个`webserver`挂掉了怎么办？重新分发用户请求。这个时候有高可用能力，但是不是高可用集群。为什么这么说，因为没有`health check`，前面的`primary`需要检查后面的几个`webserver`检查对方是否在线，如果某个`webserver`不在线，那么就把它踢出去，如果过一会这个`webserver`又好使了，那么再把它加回来。

   ```
服务的可用性,平均无故障时间 ：
90%，99%，99.9%，99.999% (mission critical)
冗余
   ```

   可以让两台主机都在线，但是两台主机提供不同的服务，这样可以减少服务器的浪费。比如原来我要做前段页面的高可用我搞了两台服务器，同时我要做邮件的高可用，那么假设如果一个高可用需要两台服务器，那么总共就需要四台服务器。这样始终有两台服务器是空闲的。这就是一种浪费。所以我们可以这么做，只使用两台服务器，其中一台跑前端页面的高可用，另外一台跑邮件的高可用。但是两台服务器上都部署了前端页面服务和邮件服务，这样如果前端页面的那台挂了就把服务切换到邮件服务器上，原来的邮件服务器上面就跑了两种服务，既有前端页面高可用，又有邮件的高可用。两个节点都起了服务，但是起的服务不同。

​    心跳是通过使用交换机，多播或者组播实现的。

​    在高可用集群之间传递的不光有心跳信息，而且还有集群事务信息，`web`服务器有运行的优先级，最亲近的主机不在线了，往哪个主机转发，集群里面必须有一个负责监督，一定要有一个主的，必须有一个负责协调整个集群完成各种事务的节点，家有千口主事一人，所以我们需要一个`DC`，推选的事务协调员。如果`DC`挂了，那么重新推选一个。

```
共享存储分类：
（1）DAS：Direct Attached Storage   直接附加存储
 用户在访问真正对应的资源的时候，是由内核直接操纵的块设备。通过驱动去操作的。如果有两个主机访问同样的一个设备，都使用DAS方式连接上来，会出现什么问题？比如两个进程在同样的一个主机试图去写一个文件，这是不行的，所以可以给文件加锁，但是如果这个两个进程位于不同的主机，都使用DAS方式访问这个文件， 写文件的时候是两个不同的主机把文件从共享存储上加载到自己的内存当中，然后各自读写各自的，然后写完了再重写写入到磁盘上。如果在不同的主机上写，那么两个主机感知不到对方的存在，所以这样就会发生写入的冲突。
 
（2）NAS：Network Attached Storage  网络附加存储
（teacher说的是NFS，但是笔记确放在了NAS这里）
在NFS上两个主机同时写一个文件，会不会导致文件错乱？NFS服务器有自身的操作系统，所以第一个主机在进行写文件的时候，为了避免其他主机对文件的修改，所以会让NFS服务器给文件加锁的。所以第二个主机来试图写同样的一个文件的时候，NFS服务器会拒绝加锁的。
（3）NFS, SAMBA                         
在NFS上两个主机同时写一个文件，会不会导致文件错乱？NFS服务器有自身的操作系统，所以第一个主机在进行写文件的时候，为了避免其他主机对文件的修改，所以会让NFS服务器给文件加锁的。所以第二个主机来试图写同样的一个文件的时候，NFS服务器会拒绝加锁的。

（4）SAN: Storage Area Network
```

`split-brain`第二个节点认为第一个节点挂了，但是实际上第一个节点没有挂，那么两个一起往一个文件里面写数据，这样就出现问题了，这种现象就叫做脑裂，左右不协调了。那么怎么解决，就是第二个节点发现第一个节点竟然没有挂掉，那好你没挂掉，我帮你挂掉，一刀捅死你。这样第二个节点就是主了，然后第二个节点开始正常工作。怎么做这件事，因为两者都连接在同一个`电源交换机`上，那么我告诉电源交换机给你断电，这个就叫做`STONITH：Shoot The Other Node In The Head`爆头。但是这种方式比较残忍，可以采用另外一种方式，比如第一个节点需要访问数据库，或者其他的某种资源，那么我第二个节点可以不让你访问这个资源，这种拒绝访问某种资源的方式叫做`fencing：隔离`。

```
fencing：隔离有两种级别的：
（1）节点级别的    STONITH
（2）资源级别的
```

为了避免集群分裂，高可用集群至少要有三个节点，而不是两个，至少要有奇数个节点，这样就可以通过冲裁机制判断。

```
Cluster分类：
（1）LB：
并发处理能力

（2）HA：High Availability，高可用
在线时间/(在线时间+故障处理时间)
99%, 99.9%, 99.99%, 99.999%
（3）
HP(HPC): 比如Hadoop
High Performance
3.1 向量机 ：   比如一个主板上添加非常多的CPU，如果CPU多了，主板设计的不够好，那么总会出问题
3.2 并行处理集群 ： 分布式文件系统，将大任务切割为小任务，分别进行处理的机制

```

接下来就开始详细的将上面的三种集群：

## 三、负载均衡集群

负载均衡集群要有对后端主机进行健康状况检查的功能，如果一个主机不正常了，那么就不要将信息转发到这个主机。

```
Hardware
        F5公司的BigIP，1000w
        IBM公司的A10, 600w
        Citrix公司的Netscaler, 500w
Software
	四层
		LVS (Linux Virtual Server)
	七层(七层设备一般称为代理)
        Haproxy：能处理http,tcp(mysql,smtp),
        Nginx：能处理http,smtp,pop3,imap

Varnish, squid

```

### 3.1  LVS

`LVS`本身也是工作在`linux`内核上的`tcp,ip`协议栈的。借鉴了`netfilter`的链，也就是`iptables`。`LVS`工作在`INPUT`链上。`LVS`会强行修改一个数据报文的行程，本来一个数据报文过了INPUT链，会到用户空间去了，但是`LVS`改变了这个流程，当一个报文走到`INPUT`链上去了之后，一旦发现用户请求的是一个集群服务。`LVS`会强行修改这个流程，把这个报文送到了`FORWARD`然后转发到了`POSTROUTING`。然后转发到其他的主机。所以`LVS`是工作在`INPUT`链上的。这个和正常的`IPTABLES`机制反常。所以有一个要求就是`LVS`和`IPTABLES`不能同时使用。
    
我们的`iptables\netfilter`是一个两段式的工具，`iptables`只是一个用来写规则的工具，我们的`netfilter`是一个框架，才是真正的规则生效的地方。我们的`LVS`也是类似的两段式的东西，我们的规则是用`ipvsadm`写的，而实际上生效的还是`ipvs`。`ipvsadm`是一个管理工具，管理集群服务的命令行工具，工作在用户空间上，而`ipvs`才是工作在内核上的。并且监控工作在`INPUT`链上的框架。首先在用户空间使用`ipvsadm`写一堆的规则，然后这堆规则被送给内核空间的`ipvs`上，而我们的`ipvs`是结合`INPUT`链发挥功能的。当一个数据包到达了`INPUT`链，那么`ipvs`就谱上来进行检查。当`ipvs`发现这个数据包是一个集群的请求，那么就把这个数据包的请求直接转发到`FORWARD`上面。然后继续转发到`FORWARD`,然后转发到其他主机。早期版本没有内置`ipvs`，较早的版本需要自己安装。



36_02_Linux集群系列之二——LVS类型详解  14分钟









​    







## 参考

- [参考文献1、DNS中的七大资源记录介绍]( https://blog.csdn.net/weixin_41545330/article/details/80865676 )
- [参考文献2、Rsync+Inotify实时同步环境部署记录](https://www.cnblogs.com/kevingrace/p/6001252.html)
- 

