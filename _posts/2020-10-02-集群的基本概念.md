[TOC]



## 一、服务器性能问题的引入

​    假设我们使用`LAMP`搭建了一个网站，并发状态的请求有`200`个,而其中的 `50`个请求是 `dynamic（动态的）`那么我们估算它需要多少资源，假设我们的`apache`工作在`prefork`模式下，大概每一个进程所占用的内存空间应该有`2M`，涉及到的动态进程，一个动态进程内存空间大概是`10M`左右。`500M+150*2M`（也就是50\*10+150\*2）所以大致需要`800M`的内存，这`50`个动态进程，每个在执行的时候可能和本地的`mysql`进行交互，`mysql`是`cup`密集型的操作，而且进程切换本身也需要`cpu`，所以我们配置了一个2个`cpu`和`4g`内存，另外假设`200`个请求同时到达，那么我们的网卡要开始进行处理，还是需要`cpu` 。因为到达网卡上的`http`请求，是通过硬件网卡上来的，所以首先要到达内核空间，但是我们的`web`服务器是运行在用户空间的，所以需要内核空间进行一定的处理传递给用户空间的`web`服务器。

   因为有`200`个并发请求，那么意味着随时随地可能有`200`个`IO`。而几乎每一个`IO`都需要有`CPU`的参与来执行的。而且每个用户的请求数据都位于磁盘上。磁盘上的数据只有被操作系统内核载入到内核空间才能通过`web`服务器经过网卡发送给用户，所以`web`请求的背后，会涉及到大量的网络`IO`和磁盘`IO`。而这些不可避免的会影响我们网络服务器的性能的。假如网站越来越受到环境，并发请求变成了`1000`，那么其中有动态请求是`200`。什么是动态请求，什么是静态请求，比如请求`html，css，img`等等这些都是静态请求，需要服务器程序运行的就是动态请求。动态请求的执行速度很慢的，比静态请求慢。那么这个时候需要的资源就是`800*2+200*10`，那么这个时候资源开始吃紧了。我们还是两个`cpu`但是我们有`1000`个进程，所以一个`cpu`上会有500个进程进行切换，那么这个时候光进行切换，就需要很多的资源，所以我们的服务器应该相当的吃力了。

​    还有假如我们的带宽上线只有`400`个进程，那么另外的`600`个都需要进行排队。这个时候用户体验就会非常的差。研究发现一个网站打开时间超过`3s`就会流失`60%`的用户。



## 二、解决方法

（1）`Scale on/up向上扩展` 

​     就是换服务器，原来是`4g`内存，2个`cpu`，那么我们换成`16g`内存，`8`个`cpu`。所以通过调换一台有这更强处理能力的服务器，这个就是`Scala on`。但是一般来说尽管变成了`16g`内存和`8`个`cpu`。但是我们带来的性能并不是原来的`4`倍。这个不是线性的，而是类似于sin(x)的走到顶峰就会下降。`8`个cpu会产生争夺，需要仲裁。仲裁本身还需要资源，这个需要的资源代价也很大。另外硬件的价格非常的贵。而且当请求变成`10万`，你能找到这样的服务器吗？

（2）`Scala out向外扩展`

​    一台服务器如果能处理`200`那么我就多加上几台，这样不就能处理`1000`了吗？但是问题来了，我们怎么能将用户的请求分到三台不同的服务器上。所以引入了负载均衡！

比如使用`DNS`的`A`记录（见参考文献1）有如下：

```shell
www.baidu.com.cn     IN     A   172.100.120.1
www.baidu.com.cn     IN     A   172.100.120.2
www.baidu.com.cn     IN     A   172.100.120.3
```

​    那么不同的用户请求来了，`DNS`会返回不同的`IP`所以这样就可以访问不同的服务器地址了。但是我们不能简单的使用`DNS`来进行负载均衡。为什么，**因为`DNS`是有缓存的**！**这样就会导致用户始终访问一个服务器，就起不到负载均衡的效果了。**

   怎么能让用户的请求，公平的平均的分配到各个服务器？所以我们需要一个在服务器前面放一个特殊设备，当请求来了，到达这个特殊设备，由这个特殊设备来把请求分配给后面的服务器。最简单的调度算法就是轮训（`RR`）。

   一个浏览器可以并发的起多个`http`请求，比如`chrome`应该是`6`个假设这`6`个请求分配到不同的服务器上了，那么就同时进行了请求，那么速度就非常的快。所以我们在浏览器中请求的对象可能来自多台不同的服务器。

调度算法有：

```
RR   轮训
WRR  加权轮训
```

![高可用-最初架构](https://github.com/Christian-health/christian-health.github.io/blob/master/img/%E9%AB%98%E5%8F%AF%E7%94%A8-%E6%9C%80%E5%88%9D%E6%9E%B6%E6%9E%84.jpg?raw=true)  

​     架构的最前面放置一个负载均衡器，将到来的`http`请求分派给不同的`webserver`，然后为了避免一个用户在第一个服务器上写了一个文章，然后再次连接的时候，它的请求被分配到第二个服务器上了，就看不到刚刚写的文章了，那么为了解决这个问题，就需要让三个服务器访问同样的一个数据库，所以在`webserver`后面添加了个数据库。但是图片是不能存放到数据库中的，数据库不能存放图片，那么这个图片放在硬盘上的一个目录里面，那么把这个文件的连接存放到数据库里面，这样就实现了附件图片的上传。所以又需要让三个服务器访问同样的一个存放文件的硬盘空间。所以添加了一个`NFS`也就是网络文件系统。这样三个服务器都可以访问到图片了。

​     **页面文件`html`和`css`存放在哪里？直接放在服务器本地**，三个服务器都放置上。但是如果过几天服务器上的静态页面需要变更添加新功能，只有三台还好弄，但是如果是`30`台，怎么办？难道都要手动的换？所以需要使用`rsync`就是一个文件的同步复制工具。`rsync`是一个高效的，远程复制工具，能实现增量复制。那么怎么及时的通知其他的服务器来同步数据那 ？我们有`rsync+inotify`（见参考文献2）。`rsync`的读音是`r,si:nk`。 文件内容发生改变是内核空间监控的，只有内核空间把变更发给用户空间，用户才能知道文件发生了改变。

​    还有工作在被动模式下的`ftp`，前端的每一个大于`1024`端口的请求都要代理到后端主机上去。还有两台`mysql`如何进行更新。

​    还有`loadbalance`，如果我们的网站的规模越来越大，那么我们的`loadbalance`的性能早晚会达到上限，那么怎么办？这个时候不能采用在前面添加多个`loadbalance`的方法，因为多个`loadbalance`前面加上一个`loadbalance`又形成了新的性能瓶颈。所以这个时候要使用其他的方法解决这个问题，**这个解决方法就是：功能切分**。把一个大的功能切分成小的功能，每一个小的功能做成一个集群。比如一个`www.sina.com`我在这个页面上点击新闻，然后发现跳转到`www.news.sina.com`这个网站上面了。这就是把一个大的页面的功能拆分了。所以我点击新浪上的某一个标签，也就是某一个功能，请求就可能被分发到另外的一个集群上，每一个小的功能，都在一个集群上。看他们所有标签在同一个主页上，但是背后的服务器集群确是不同的。

![HA集群](https://github.com/Christian-health/christian-health.github.io/blob/master/img/HA%E9%9B%86%E7%BE%A4.jpg?raw=true) 

   如上图，如果调度器`loadbalance`挂掉了，怎么办？所以我们需要**一个`standby（备用的）`调度器**。主备之间有一个`heartbeat`心跳，如果备份节点发现心跳断掉了，那么就启动。如果某个`webserver`挂掉了怎么办？重新分发用户请求。这个时候有高可用能力，但是不是高可用集群。为什么这么说，因为没有`health check`，前面的`primary`需要检查后面的几个`webserver`检查对方是否在线，如果某个`webserver`不在线，那么就把它踢出去，如果过一会这个`webserver`又好使了，那么再把它加回来。

   ```
服务的可用性,平均无故障时间 ：
90%，99%，99.9%，99.999% (mission critical关键业务)
冗余
   ```

   可以让两台主机都在线，但是两台主机提供不同的服务，这样可以减少服务器的浪费。比如原来我要做前段页面的高可用我搞了两台服务器，同时我要做邮件的高可用，那么假设如果一个高可用需要两台服务器，那么总共就需要四台服务器。这样始终有两台服务器是空闲的。这就是一种浪费。所以我们可以这么做，只使用两台服务器，其中一台跑前端页面的高可用，另外一台跑邮件的高可用。但是两台服务器上都部署了前端页面服务和邮件服务，这样如果前端页面的那台挂了就把服务切换到邮件服务器上，原来的邮件服务器上面就跑了两种服务，既有前端页面高可用，又有邮件的高可用。两个节点都起了服务，但是起的服务不同。

​    **心跳是通过使用交换机，多播或者组播实现的。**

​    **在高可用集群之间传递的不光有心跳信息，而且还有集群事务信息，**`web`服务器有运行的优先级，最亲近的主机不在线了，往哪个主机转发，集群里面必须有一个负责监督，一定要有一个主的，必须有一个负责协调整个集群完成各种事务的节点，家有千口主事一人，所以我们需要一个`DC`，推选的事务协调员。如果`DC`挂了，那么重新推选一个。

```
共享存储分类：
（1）DAS：Direct Attached Storage   直接附加存储
 用户在访问真正对应的资源的时候，是由内核直接操纵的块设备。通过驱动去操作的。如果有两个主机访问同样的一个设备，都使用DAS方式连接上来，会出现什么问题？比如两个进程在同样的一个主机试图去写一个文件，这是不行的，所以可以给文件加锁，但是如果这个两个进程位于不同的主机，都使用DAS方式访问这个文件， 写文件的时候是两个不同的主机把文件从共享存储上加载到自己的内存当中，然后各自读写各自的，然后写完了再重写写入到磁盘上。如果在不同的主机上写，那么两个主机感知不到对方的存在，所以这样就会发生写入的冲突。
 
（2）NAS：Network Attached Storage  网络附加存储
（teacher说的是NFS，但是笔记确放在了NAS这里）
在NFS上两个主机同时写一个文件，会不会导致文件错乱？NFS服务器有自身的操作系统，所以第一个主机在进行写文件的时候，为了避免其他主机对文件的修改，所以会让NFS服务器给文件加锁的。所以第二个主机来试图写同样的一个文件的时候，NFS服务器会拒绝加锁的。
（3）NFS, SAMBA                         
在NFS上两个主机同时写一个文件，会不会导致文件错乱？NFS服务器有自身的操作系统，所以第一个主机在进行写文件的时候，为了避免其他主机对文件的修改，所以会让NFS服务器给文件加锁的。所以第二个主机来试图写同样的一个文件的时候，NFS服务器会拒绝加锁的。

（4）SAN: Storage Area Network
```

`split-brain`第二个节点认为第一个节点挂了，但是实际上第一个节点没有挂，那么两个一起往一个文件里面写数据，这样就出现问题了，这种现象就叫做脑裂，左右不协调了。那么怎么解决，就是第二个节点发现第一个节点竟然没有挂掉，那好你没挂掉，我帮你挂掉，一刀捅死你。这样第二个节点就是主了，然后第二个节点开始正常工作。怎么做这件事，因为两者都连接在同一个`电源交换机`上，那么我告诉电源交换机给你断电，这个就**叫做`STONITH：Shoot The Other Node In The Head`爆头**。但是这种方式比较残忍，可以采用另外一种方式，比如第一个节点需要访问数据库，或者其他的某种资源，那么我第二个节点可以不让你访问这个资源，这种拒绝访问某种资源的方式**叫做`fencing：隔离`**。

```
fencing：隔离有两种级别的：
（1）节点级别的    STONITH
（2）资源级别的
```

为了避免集群分裂，高可用集群至少要有三个节点，而不是两个，至少要有奇数个节点，这样就可以通过冲裁机制判断。

```
Cluster（集群）分类：
（1）LB：
并发处理能力
（2）HA：High Availability，高可用
在线时间/(在线时间+故障处理时间)
99%, 99.9%, 99.99%, 99.999%
（3）高性能集群，解决复杂计算的。
HP(HPC): 比如Hadoop
High Performance
3.1 向量机 ：   比如一个主板上添加非常多的CPU，如果CPU多了，主板设计的不够好，那么总会出问题
3.2 并行处理集群 ： 分布式文件系统，将大任务切割为小任务，分别进行处理的机制

```

接下来就开始详细的将上面的三种集群：

## 三、负载均衡集群

负载均衡集群要有对后端主机进行健康状况检查的功能，如果一个主机不正常了，那么就不要将信息转发到这个主机。

```
Hardware
        F5公司的BigIP，1000w
        IBM公司的A10, 600w
        Citrix公司的Netscaler, 500w
Software
	四层
		LVS (Linux Virtual Server)
	七层(七层设备一般称为代理)
        Haproxy：能处理http,tcp(mysql,smtp),
        Nginx：能处理http,smtp,pop3,imap

Varnish, squid

这里记录一下，将来要对这些概念有一个了解！！

```

### 3.1  LVS

`LVS`本身也是工作在`linux`内核上的`tcp,ip`协议栈的。借鉴了`netfilter`的链，也就是`iptables`。`LVS`工作在`INPUT`链上。`LVS`会强行修改一个数据报文的行程，本来一个数据报文过了`INPUT`链，会到用户空间去了，但是`LVS`改变了这个流程，当一个报文走到`INPUT`链上去了之后，一旦发现用户请求的是一个集群服务。`LVS`会强行修改这个流程，把这个报文送到了`POSTROUTING`。然后转发到其他的主机。所以`LVS`是工作在`INPUT`链上的。这个和正常的`IPTABLES`机制反常。所以有一个要求就是`LVS`和`IPTABLES`不能同时使用。
    
我们的`iptables\netfilter`是一个两段式的工具，`iptables`只是一个用来写规则的工具，我们的`netfilter`是一个框架，才是真正的规则生效的地方。我们的`LVS`也是类似的两段式的东西，我们的规则是用`ipvsadm`写的，而实际上生效的还是`ipvs`。`ipvsadm`是一个管理工具，管理集群服务的命令行工具，工作在用户空间上，而`ipvs`才是工作在内核上的。并且监控工作在`INPUT`链上的框架。首先在用户空间使用`ipvsadm`写一堆的规则，然后这堆规则被送给内核空间的`ipvs`上，而我们的`ipvs`是结合`INPUT`链发挥功能的。当一个数据包到达了`INPUT`链，那么`ipvs`就谱上来进行检查。当`ipvs`发现这个数据包是一个集群的请求，那么就把这个数据包的请求直接转发到`FORWARD`上面。然后继续转发到`FORWARD`,然后转发到其他主机。早期版本没有内置`ipvs`，较早的版本需要自己安装。

![lvs各种ip说明](https://github.com/Christian-health/christian-health.github.io/blob/master/img/lvs%E5%90%84%E7%A7%8Dip%E8%AF%B4%E6%98%8E.jpg?raw=true)  

```
(1) Virtual IP (VIP) address 
The IP address the Director uses to offer services to client computers

(2)Real IP (RIP) address 
The IP address used on the cluster nodes

(3)Director's IP (DIP) address 负载均衡器称为Director
The IP address the Director uses to connect to the D/RIP network

(4)Client computer's IP (CIP) address 客户端的IP
The IP address assigned to a client computer that it uses as a source IP address for requests sent to the cluster

```

`LVS`有三种类型“

```
LVS clusters are usually described by the type of forwarding method the LVS Director uses to relay incoming requests to the nodes inside the cluster. 
Three methods are currently available:
（1）Network address translation (LVS-NAT) 地址转换
（2）Direct routing (LVS-DR)  直接路由
（3）IP tunneling (LVS-TUN)  隧道

```

#### （3.1.1）`LVS-NAT`地址转换规则（类似于`DNAT`） 

![mage-lvs-nat工作原理](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-lvs-nat%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.jpg?raw=true)

看到这里我们可以发现，负载均衡器既要修改客户请求的数据包的目的`IP`地址，又要修改`RealServer`响应的源`IP`地址，这样对负载均衡器造成了很大的压力。所以这样的搞法，负载均衡器也就是能带动`10`个`RealServer`撑死了。而且带动了`10`个性能也很一般。

使用`LVS-NAT`方式，那么有如下的要求：

（1）所有的`RealServer`和负载均衡器`Director`必须工作在同一个网络中。

（2）所有的`RealServer`的**网关**必须指向`Director`，否则就不会经过`Director`，那么`Director`就无法修改`RealServer`响应的数据包。

（3）`RealServerIP`基本都是私有`IP`地址，只能用来和`Director`来进行通信。

（4）`Director`要负责所有`Client`和`RealServer`的通信，包括进来的和出去的。

（5）`Director`需要支持端口映射。向外提供服务的端口和`RealServer`上提供服务端口可以不一样。

比如我`RealServer`上真正提供服务的端口是`8080`，但是我的`Director`上对外提供服务器的端口是`80`端口。

那么`Director`需要提供端口映射，把`80`端口映射成为`8080`端口。

（6）`RealServer`的操作系统类型可以是任意的，比如里面的`RealServer`类型可以是`windows`，只要它能对外提供服务就行。但是`Director`的类型必须要是`Linux`，因为它要执行`LVS`

（7）较大规模的应用场景中，单独的`Director`很可能成为集群的瓶颈。

综上就是`LVS-NAT`的工作特点，但是我们真实的工作方式不会使用`LVS-NAT`，基本都是使用如下的`DR`模式。

![mage-lvs-dr-工作原理](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-lvs-dr-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.jpg?raw=true)

`LVS-DR`的工作原理如下：

当一个前端请求通过交换机到达了`Director`，那么`Director`通过算法选择一个`RealServer`然后通过交换机，将数据转发给`RealServer`。当`RealServer`做出响应之后，不是再经过`Director`了，而是直接的通过交换机，将响应返回给前端。这样就避免了`Director`的瓶颈。让`RealServer`直接返回给客户端。

![mage-lvs-dr-工作原理2](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-lvs-dr-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%862.jpg?raw=true)

前面讲过，响应的时候返回给客户端的数据包中的源`IP`地址，应该是`VirtualIP`。但是如果按照前面说的，由`RealSever`直接返回的话，那么客户端收到的数据包中的源`IP`就是`RealIp`。这样客户端没有发出针对`RealIP`的请求，那么它就不会进行处理。我们还是需要让返回给客户端的数据包中的源`IP`地址是`VirtaulIP`而不是`RealIP`那么怎么完成这个？如上图。

我们让`Director`和`RealServer`都有这个`VirtaulIP`。但是如果一个局域网内，都有`VIP`了，那么就会有地址冲突。为了不让`IP`地址冲突，有两个`IP`，一个是`VIP`，另外一个是`DIP`，**一个配置在网卡上，另外一个配置在网卡别名上**。`VIP`地址是配置在一个网卡别名上，并且是隐藏的，不用来接收数据。所以在整个网络上我广播一下谁是`VIP`啊，`RealServer`不会认为自己有`VIP`这个地址，配置是配置了，但是是隐藏的，不做通信使用，而是作为`RealServer`响应数据的时候，作为源`IP`地址使用。我虽然是使用这个`VIP`地址作为源`IP`响应这个数据。但是我实际上还是从`RIP`这个网卡设备出去的。真正通信和这个`VIP`没有任何的关系。

在本地，路由器是怎么把报文发送给`VIP`的。图上的两个设备之间通信使用的是物理层的`MAC`地址。那么从图上路由器出来的报文，怎么知道`Director`的`MAC`地址。使用`ARP`广播一下。那么每一个`RealServer`也会收到这个`ARP`消息。但是这三个`RealServer`不能响应，一定要隐藏起来。如果他们三个响应就麻烦了。所以一定要使用各种方式把它隐藏起来。只有我们`Director`上的`VIP`才会响应。这样来自客户端的请求最终都交给了`Director`了 。`DR`模型里面`Director`不改变目的`IP`地址，而只是改变`MAC`地址。如图可以看到，当一个数据包到达了`Director`，那么`Director`不会拆`IP`首部，只会拆分`MAC`首部。`Director`有自己的`DirIP`，每一个`RealServer`有自己的`RIP`，他们都是在一个网段里面，所以他们直接是可以通信的。所以`Director`收到一个报文之后，需要把源`MAC`地址和目标`MAC`地址，进行更改，源`MAC`地址就是`Director`的`MAC`地址，而目标`MAC`地址就是`RealServer`的`MAC`地址。那么这个数据包被转发到了指定的`RealServer`，指定的`RealServer`上面都有`virtualIP`那么就认为自己可以接受并处理这个报文。

所以这种场景下`Director`只是处理入站点的请求，而不再处理响应请求，因为一般都是请求报文比较小，而响应报文比较大。如果`NAT`模型能带动`10`个`webServer`，那么`DR`模型能带动`100`个。

#### （3.1.2）`DR调度方式`

![mage-lvs-dr工作方式的要求](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-lvs-dr%E5%B7%A5%E4%BD%9C%E6%96%B9%E5%BC%8F%E7%9A%84%E8%A6%81%E6%B1%82.jpg?raw=true)

（1）各集群节点必须要和`Director`在同样的一个物理网络中，因为要根据`MAC`地址进行转发。

（2）`RealServerIP`不用再是私有`IP`了，

（3）`Director`只是处理入站请求，响应报文则由`RealServer`直接发给客户端。

（4）`RealServer`不能将网关指向`Director`，而直接使用前端网关。

（5）`Director`无法实现端口映射。因为只是拆掉了`MAC`层。

（6）大多数的操作系统都可以作为`RealServer`，因为`RealServer`要隐藏`VIP`

（7）`DR`模型可以比`NAT`模型高效。



`DR`模型也有自己的缺点，就是`DR`模型要求，**各集群节点必须要和`Director`在同样的一个物理网络中，因为要根据`MAC`地址进行转发。**

所以需要在同样的一个物理网段内。但是如果这样的话，我想实现异地灾备就难以实现了。所以引入了第三种方式就是`TUN`也就是隧道。、

#### （3.1.3）`TUN调度方式`

`TUN`模型：

隧道的工作原理和`DR`模型一样。不通的是需要重新封装报文。

`Director`和`RealServer`不在一个`IP`范围内。那么怎么解决这个问题那，使用如下的方式解决这个问题：

![mage-lvs-tun隧道工作原理](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-lvs-tun%E9%9A%A7%E9%81%93%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.jpg?raw=true)

借助一个`IP`报文，发送另外一个`IP`报文。这就技术就是隧道技术。

![mage-lvs-tun工作方式的要求](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-lvs-tun%E5%B7%A5%E4%BD%9C%E6%96%B9%E5%BC%8F%E7%9A%84%E8%A6%81%E6%B1%82.jpg?raw=true)

（1）集群节点可以跨越互联网

（2）`RealServer`的`RIP`必须是公网地址

（3）`Director`只是处理入站请求

（4）`RealServer`和`Director`必须要支持隧道i技术

（5）响应报文一定不能通过`Director`

（6）不支持端口映射

#### （3.1.4）`LVS`调度算法：

（1）静态调度算法

1、`rr` 轮询调度算法

2、`wrr` 加权轮询调度算法

3、`sh` `source hash`调度算法 ，这种方法在一定程度上破坏了负载均衡。但是为什么还要有这种调度算法，因为

`http`协议是无状态的，所以每次的请求都被视为是一个新的请求，所以有`cookie`和`session`。客户端第一次访

问服务器的时候，服务器会生成一个标识。并且把这个标识发送给客户端。客户端把这个东西保存到`cookie`

，以后客户端在发送请求的时候，都会把`cookie`发送给服务端。进而在服务端，只要两次请求带来的`cookie`

都是同样的一个`cookie`那么，服务端就知道这两个请求是同样的一个客户端发送来的。原来把一些用户的信息放

在客户端的`cookie`里面，后来放在服务端了。`session`有过期时间，比如这是一个电商网站，那么客户在购物车

里面，添加了一个新买的商品，那么这个商品就被保存到一个服务器的`sesion`中了。如果不使用`source hash`算

法，那么下次这个客户端的请求，就可能被发送到另外一台服务器上了，那么这个另外服务器上的`session`中就

没有刚刚客户添加的商品。所以这样就是有问题的。所以就需要把一个客户的所有请求发送到同样的一台服务器

上，所以就有了`source hash`这种调度算法。所以`source hash`这种调度算法最主要的就是用来实现`session `

`affinity`也就是会话绑定。能够让用户以前建立的`session`信息继续使用的。

![mage-共享session](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-%E5%85%B1%E4%BA%ABsession.jpg?raw=true)

**所以如果不能实现`session`共享，那么就需要使用`source hash `。但是如果我使用了`session`共享那么就可以不使用`source hash`**

4、`DH（Destination hash）`  

把同一个`IP`地址的请求，发送给同样的一个`server`。和`source hash`功能是类似的，适用的场景不通，算法也不通。

比如，有一个客户端发出了一个请求到`Director`，而`Director`后面是两个缓存。那么这两个缓存最开始是没有

请求的数据的，那么就要去持久化的服务器就获取，获取之后，先要存放到缓存中，然后再发送给客户端，然后另

外客户端下次又来了一个请求，也是请求刚才的那个客户端要请求的数据，那么最好的方式就是把这个请求转发给

刚刚的那个缓存，如果转发给其他的缓存就会出现缓存又没有命中的情况。所以就会有这种`DH`调度算法。

为什么上面的调度算法叫做静态调度算法，因为他们不考虑当前服务器的繁忙情况。就是规则制定好了，就这样执

行，不智能。会出现忙闲不均的情况。

活动链接：用户请求来了，并且在传输数据。

非活动链接：链接建立，数据传输已经结束，但是链接并未断开。

活动链接占用的资源较多。所以有时候我们可以无视非活动链接。因为它占用的资源很少。

![mage-dhsh算法详解](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-dhsh%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3.jpg?raw=true)

（2）动态调度算法

考虑非活动的连接数，来进行调度。

1、`lc(least connect)`最少连接

最少连接，看后端服务器的活动链接数，`active*256+inactive`谁小，那么就把请求发送给它。

2、`wlc` 加权最少连接

`(active*256+inactive)/权重`谁的小，那么就要谁。

这种也有问题，就是最开始的时候，根据计算公式，会把请求发送给权重低的性能差的服务器。

所以有如下算法改进。

3、`sed (short efire delay)`最少期望延迟上面的

`(active+1)*256/权重`

这种算法也有问题，就是权重小的服务器，别人那里好多请求了，但是它那里一个也没有，于是有了`NQ`

4、`NQ（never queue）`调度算法

根据权重来发，就算权重小，也会给你一个请求。

5、`LBLC`基于本地的最少连接调度算法

主要的实现目标和`DH`一样。`DH`不考虑当前`cache`的连接数，但是`LBLC`要考虑当前`cache`的连接数。

可能会破坏`cache`命中率。`LBLC`是动态的`DH`算法。

6、`LBLCR`

基于本地的，带**复制功能**的最少连接。

`LBLCR`只要是一个新请求，那么就会发给最少的那个`cache Server`，因为两个`cache server`实现了复制功能。

缓存复制机制，兄弟服务器。旧的请求还是发送到旧的服务器上去，而新的请求才发送到最少的那个服务器上去。

这种缓存复制不是两个缓存服务器上的数据一模一样，而是比如`A`服务器上有，但是`B`服务器上没有，那么`B`服务

器可以去`A`服务器上去要。不是所有的都同步过来。



####    (3.1.5)   调度算法总结：

到底为止，`10`种调度算法，`4`种静态的，`6`种动态的。生产环境中哪种是最好的？我们的活动链接数和非活动链

接数可能都很大。不考虑特殊场景，不考虑缓存服务器，只考虑负载均衡调度算法，哪种是最好的？`wlc`

关于这`10`种类的调度算法，在`lvs`官方网站上有详细的解释。我们可以去看参考文献3

####     (3.1.6)   `LVS`调度实战：

```
/boot  存放开机时所要用的文件，包括linux核心文件、开机菜单和开机所要的配置文件。
当前Linux系统的kernel配置文件，可以使用下面指令查看。
cat /boot/config-3.10.0-862.el7.x86_64 | more
查看这个文件，我们就可以看到这个linux操作系统到底是否支持lvs功能
```

```
IPVS就是LVS相关的。
[root@yxf hot]# grep -i 'vs' /boot/config-3.10.0-862.el7.x86_64 
CONFIG_GENERIC_TIME_VSYSCALL=y
# CONFIG_X86_VSMP is not set
CONFIG_NETFILTER_XT_MATCH_IPVS=m
CONFIG_IP_VS=m
CONFIG_IP_VS_IPV6=y
# CONFIG_IP_VS_DEBUG is not set
CONFIG_IP_VS_TAB_BITS=12
# IPVS transport protocol load balancing support
CONFIG_IP_VS_PROTO_TCP=y
CONFIG_IP_VS_PROTO_UDP=y
CONFIG_IP_VS_PROTO_AH_ESP=y
CONFIG_IP_VS_PROTO_ESP=y
CONFIG_IP_VS_PROTO_AH=y
CONFIG_IP_VS_PROTO_SCTP=y
# IPVS scheduler   LVS支持的调度算法  这里叫做IPVS    
CONFIG_IP_VS_RR=m
CONFIG_IP_VS_WRR=m
CONFIG_IP_VS_LC=m
CONFIG_IP_VS_WLC=m
CONFIG_IP_VS_LBLC=m
CONFIG_IP_VS_LBLCR=m
CONFIG_IP_VS_DH=m
CONFIG_IP_VS_SH=m
CONFIG_IP_VS_SED=m
CONFIG_IP_VS_NQ=m
# IPVS SH scheduler
CONFIG_IP_VS_SH_TAB_BITS=8
# IPVS application helper
CONFIG_IP_VS_FTP=m
CONFIG_IP_VS_NFCT=y
CONFIG_IP_VS_PE_SIP=m
CONFIG_OPENVSWITCH=m
CONFIG_OPENVSWITCH_GRE=m
CONFIG_OPENVSWITCH_VXLAN=m
CONFIG_OPENVSWITCH_GENEVE=m
CONFIG_VSOCKETS=m
CONFIG_VSOCKETS_DIAG=m
CONFIG_VMWARE_VMCI_VSOCKETS=m
CONFIG_VIRTIO_VSOCKETS=m
CONFIG_VIRTIO_VSOCKETS_COMMON=m
CONFIG_HYPERV_VSOCKETS=m
CONFIG_MTD_BLKDEVS=m
CONFIG_SCSI_MVSAS=m
# CONFIG_SCSI_MVSAS_DEBUG is not set
CONFIG_SCSI_MVSAS_TASKLET=y
CONFIG_VMWARE_PVSCSI=m
CONFIG_VSOCKMON=m
CONFIG_VHOST_VSOCK=m
CONFIG_MOUSE_VSXXXAA=m
CONFIG_MAX_RAW_DEVS=8192
# CONFIG_POWER_AVS is not set
CONFIG_USB_SEVSEG=m
[root@yxf hot]# 

```

前面已经看到内核已经支持`LVS`,那么这里我们只需要安装`ipvsadm`就可以了。

```
安装ipvsadm的方法如下：

[root@yxf hot]# yum install ipvsadm
已加载插件：fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * base: mirrors.ustc.edu.cn
 * extras: mirrors.huaweicloud.com
 * updates: mirrors.aliyun.com
base                                                                                                                                                            | 3.6 kB  00:00:00     
extras                                                                                                                                                          | 2.9 kB  00:00:00     
updates                                                                                                                                                         | 2.9 kB  00:00:00     
正在解决依赖关系
--> 正在检查事务
---> 软件包 ipvsadm.x86_64.0.1.27-8.el7 将被 安装
--> 解决依赖关系完成

依赖关系解决

=======================================================================================================================================================================================
 Package                                     架构                                       版本                                            源                                        大小
=======================================================================================================================================================================================
正在安装:
 ipvsadm                                     x86_64                                     1.27-8.el7                                      base                                      45 k

事务概要
=======================================================================================================================================================================================
安装  1 软件包

总下载量：45 k
安装大小：75 k
Is this ok [y/d/N]: y
Downloading packages:
ipvsadm-1.27-8.el7.x86_64.rpm                                                                                                                                   |  45 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  正在安装    : ipvsadm-1.27-8.el7.x86_64                                                                                                                                          1/1 
  验证中      : ipvsadm-1.27-8.el7.x86_64                                                                                                                                          1/1 

已安装:
  ipvsadm.x86_64 0:1.27-8.el7                                                                                                                                                          

完毕！
[root@yxf hot]# 

```

`ipvsadm`和`iptables`的工作原理基本相同。可以`man ipvsadm`查看使用方法。

`ipvsadm`功能：

（1）管理集群服务

​			添加`-A|E -t|u|f service-address [-s scheduler]`

​					`-t` 表示`tcp`协议 ，如果要是定义一个`web`服务器，那么就使用`-t tcp`

​					`-u`表示`udp`   所以你要定义的如果是一个`DNS`集群，那么就是用`-u udp` 参数

​								如果是`tcp或者udp`那么`service-address`就是 `IP:PORT`

​					`-f` 防火墙标记，简称`fwm(fire ware mark)`，防火墙标记，讲到`lvs`持久连接的时候在说

​								如果是`-f`那么就是`mark number`标记号码，

​					`-s`是调度算法，默认是`wlc`        

```shell
如何定义一个集群服务：
ipvsadm -A -t 172.16.100.1:80 -s rr
服务背后没有任何的realserver，所以我们还要继续给它添加realserver。也就是使用下面的（2）管理集群服务中的real server进行添加
```

​			编辑 `-E (edit)`

​			删除 `-D (delete)`

​					`-D -t|u|f service-address`  `-D`参数很简单，使用`-t -u -f`加上`service-address`就可以了。

（2）管理集群服务中的`real server`

​    		添加 ` -a  (add)`

​						前面我们添加了一个集群服务，添加完了集群服务之后，我们还没有添加真正的`realserver`那么这个时候我们需要使用，这里的“管理集群服务中的`realserver`”这个部分，来给集群添加真正的`realservere`。

```\
  ipvsadm -a|e -t|u|f service-address -r server-address
               [-g|i|m] [-w weight] [-x upper] [-y lower]
看这些参数，可以看到有service-address这个参数，这个参数就是service-address也就是前面我们使用`ipvsadm -A -t 172.16.100.1:80 -s rr`定义的集群，所以我们添加realserver的时候，必须已经有集群了。
-r  server-address：某RS的地址，在NAT模型中，可使用IP:PORT实现端口映射。
[-g|i|m]: lvs类型有三种
（1）-g  对应DR模型
（2）-i  对应TUN模型
（3）-m  对应NAT模型
[-w weight]  权重，如果算法不支持权重，那么加上了权重也没有任何意义

ipvsadm -a -t 172.16.100.1:80 -r 192.168.10.8 -m
ipvsadm -a -t 172.16.100.1:80 -r 192.168.10.9 -m
```

​			编辑 `-e (edit)`

​			删除 `-d (delete)`

```
 ipvsadm -d -t|u|f service-address -r server-address
从一个集群中删除一个realserver
```

（3）查看

​		`-L|l`

```shell
-n  数字格式显示，不反解IP地址和端口

[root@yxf hot]# ipvsadm -L -n
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
[root@yxf hot]# ipvsadm -L -n --stats  #统计信息，就会多出Conns   InPkts  OutPkts  InBytes OutBytes几个列
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes
  -> RemoteAddress:Port
[root@yxf hot]# ipvsadm -L -n --rate
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port                 CPS    InPPS   OutPPS    InBPS   OutBPS
  -> RemoteAddress:Port

--stats:统计数据
--rate:速率
--timeout:显示tcp，tcpfin和udp的会话超时时长。
-c:显示当前的ipvs连接状态
```

删除所有集群服务：

`-C` 清空`ipvs`规则

保存规则：

`-S`

`# ipvsadm -S >/path/to/somefile`

载入此前的规则：

`-R`

`#ipvsadm -R < /path/fro/somefile`



在使用`lvs`集群的时候，各节点的时间必须要同步，如果时间差值超过了`1s`那么这个时候就会出现问题。所以我们要使用`NTP`来进行时

间同步。我们可以把`Director`做成时间服务器，那么各个节点都到时间服务器上去同步时间。

`NTP:Network Time Protocol`

我们平时对表的时候，如果发现自己的时间不准确，比如慢了一个小时，那么就把我们的表往前拨快一个小时就行了。但是这样可能会造

成服务器中的数据跨越了一个小时，也就是一个小时没有了，比如现在9点，往后拨了一个小时，那么就变成了10点，中间的时间就没有

数据了，这样做是有问题的。所以`NTP`就采用了，另外一种方式，比如原来一分钟需要走一个格子的长度（时间也是长度）。那么现在就

让它半分钟走一个格子的长度。这样快走就不会出现空白，没有时间的问题了。

```
ntpdate 10.0.0.1  这样就以10.0.0.1调整自己的时间
```

我们也可以让`NTP`自动同步，使用如下方式：

```shell
service ntpd start
vim /etc/ntp.conf

也可以在ctontab中配置定时任务来进行时间同步。
```

这里要进行NAT实验。

 1.35.40



在`DR`模型下，如何防止路由器发送信息的时候，除了`Director`响应对`VirtaulIp`的`ARP`解析请求，而让`RealServerIP`都不响应这个`ARP`解析请求。使用如下三种方法：

（1）`VIP: MAC （DVIP）`第一种方法就是在路由器上绑定好，`VIP`对应的`mac`地址就是`Director`的MAC地址。

（2）`arptables`第二种方法就是使用类似于`iptables`的`arptables`

（3）`kernel_parmeter` 使用内核参数。也就是如下：

```
arp_ignore：定义接收到的ARP请求时的响应级别
#默认级别0，只要本地配置有相应地址就会给予响应
1、仅在请求的目标地址在请求到达的接口（网卡）上的时候，才会给予响应。

arp_annouce 定义将自己地址向外通信时的通告级别
#默认级别0，表示尽可能向外通告，将本地任何接口（网卡）上的任何地址向外通告
1、试图仅向目标网络通告，与其网络匹配的接口（网卡）地址
2、仅将与本地接口匹配的网络通告
```

`kernel_parmeter` 使用内核参数，具体配置如下：

![mage-arpignoreandannouce](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-arpignoreandannouce.jpg?raw=true)

![mage-arpignore](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-arpignore.jpg?raw=true)

![mage-arp_annouce](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-arp_annouce.jpg?raw=true)

关于`arp-ignore`的详细解释：

![mage-ip属于主机](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-ip%E5%B1%9E%E4%BA%8E%E4%B8%BB%E6%9C%BA.jpg?raw=true)

**注意`IP`地址属于操作系统，属于主机，而不是属于网卡，也就是接口。**如图可以看到两个网段，左上一个`172.16.*.*`网段，右下一个网

段`10.0.*.*`网段。那么如果在`172.16.*,*`网段内有一个主机，这个主机有两个`IP`，其中一个``IP`是`172.16.100.1`另外的一个`IP`是

`10.0.0.1`，那么当整个网络刚刚通的时候，那么这台主机会广播自己的地址，让整个网络都知道自己的地址，于是这台主机会向

`172.16.*.*`这个网段广播我有两个网卡，其中一个`IP`是`172.16.100.1`。另外一个是`10.0.0.1`。于是这台主机也会向`10.0.0.*.*`这

个网段广播我有两个网卡，其中一个`IP`是`172.16.100.1`。另外一个是`10.0.0.1`。**这个信息在两个网段内会保存`300s`左右，这是`RFC`**

**文档中已经规定好的**。假如现在有一个人违规的进行操作，那么他在`10.0.*.*`这个网段内，添加了一个机器，让这个机器的`IP`地址是

`172.16.100.3`那么这个违规添加的机器能和`172.16.100.1`进行通信。能访问这个网卡，能也和这个网卡通信，但是如果想给

`172.16.100.*`网段内的其他的主机发送信息，并且通过`172.16.100.1`转发出去这是不可能的。

我们可以通过修改主机这种向外通告自己的网卡信息的这种行为，让`RealServer`不要通告自己`VIP`的这种行为。

我们每一个`RealServer`一定会有两个网卡，一个是`eth0`另外的一个就是`lo`。将我们的`RIP`配置在`eth0`上面。因为要通过`eth0`和外部

通信。**我们可以将`VIP`配置到`lo`的别名上**，比如`lo:0`。很显然`lo`接入的网络和`eth0`接入的网络不是一个网络。一旦有其他主机和我这

台主机通信，其他主机广播请求`VIP`地址，那么我这台主机就可以进行响应，因为我当前的主机是有`vip`的，`vip`配置在`lo`上面。

那么我要响应`arp`告诉请求的机器，我这里有`vip`，并且我的这个`vip`对应的`mac`地址是`eth0`这个`mac`地址。但是我们想做的是在面对

路由器的`arp`请求的时候，不要回复我这里有`vip`，并且我的这个`vip`对应的`mac`地址是`eth0`这个`mac`地址。那么怎么做，就是修改：

**响应通告级别和响应的回应级别**。**请求的目标地址（`vip`所在的lo上）没有配置在进来的（`eth0`）这个网卡上，那么就不给予响应**。**因为**

**`eth0`配置在`realServerIp`。但是`vip`在`lo`上所以就不给予响应。**

`arp-ignore`就用来定义我们对什么样的级别的`arp`请求进行响应。也就是上面这些。用来定义接收到别人请求，我的响应级别的。

`arp-annoce`就是我本机用来定义，我这个机器对应用来通告，我这个机器的`ip`对应的`mac`时候的通告级别的。



**`linux`有一种行为，报文从哪个接口出去，就尽可能使用哪个接口的`IP`地址。如果这个接口的地址和网关不在同一个网段内。它就使用这个接口别名上的地址。**也就意味着我们这个接口一定有一个地址和网关在同样的一个网段内。看下面这张图：

![mage-linux报文从哪个接口出去就尽可能使用哪个接口的IP地址](https://github.com/Christian-health/christian-health.github.io/blob/master/img/mage-linux%E6%8A%A5%E6%96%87%E4%BB%8E%E5%93%AA%E4%B8%AA%E6%8E%A5%E5%8F%A3%E5%87%BA%E5%8E%BB%E5%B0%B1%E5%B0%BD%E5%8F%AF%E8%83%BD%E4%BD%BF%E7%94%A8%E5%93%AA%E4%B8%AA%E6%8E%A5%E5%8F%A3%E7%9A%84IP%E5%9C%B0%E5%9D%80.jpg?raw=true)

图上的三个`IP`在一个网段内，那么可以看到，客户端来了一个请求。那么将来一定是`eth0`进行响应，那么返回的

就应该是`eth0`对应的`IP`地址，也就是`realServerIP2`，但是真正应该返回给用户的是`Virtual IP`。所以我们

需要进行额外的配置，让返回给客户端的响应的`IP`不是`RealServerIP2`而是`VIP`。我们只需要在当前主机上，也

就是每一个`RealServerIP`上添加一个特殊的路由配置就可以了。这个特殊的路由配置就保证，用户请求报文的地

址，一定是通过接口响应报文作为它的源地址。用户的请求报文应该到达`lo:0`那么响应报文一定要使用这个接口

`lo:0`上的地址作为响应地址。如果不限定那么一定使用`eth0`作为响应地址的，这样就出问题了。

```
37_01_Linux集群系列之四——LVS DR详解及配置演示55分钟，这里有文档，文档里面有配置的脚本，讲了DR配置的详细情况。
这里学尚硅谷的把

```



```
这里到38_01_Linux集群系列之七——高可用集群原理详解的12.16,从这之后开始讲解高可用集群
```



## 四、高可用集群

&emsp;对于负载均衡集群讲，`Realserver`可能成为我们的故障点，但是我们有`healthcheck`所以我们可以通过`healthcheck`来完成

健康状态的检测。这样就可以通过这个健康状态的检测完成切换，服务转移到其他集群节点上去。但是我们的`Director`的高可用

能力如何保证 ? 所以需要提供一个备用机的方式，用户请求通过`DNS`解析的时候只是解析到真正面向外网的`VIP`的地址。在

`Director`上把`VIP`配置成为网卡别名上的地址。万一一个`Director`故障了，那么就把只需要把这个`VIP`转移到另外的一个节点

上来。并在这个节点上启用`IPVS`规则。`VIP`完成了转移了，那么`DIP`怎么办？`DIP`用来和`RIP`进行通信。两个`DIP`不能一样。因

为两个`DIP`之间还在进行通信，他们之间为什么要进行通信，是因为两个`Director`之间还要通过通信判断对方是否还活着，还在

正常工作。心跳`heartbeat`，那么如果心脏不跳动了，那么需要使用一个`pacemaker`心脏起搏器。

![mg-高可用集群01]()

```
活动节点（主）	 被动节点
Active        Passive
Primary       Standby
```

&emsp;来自于互联网的请求，肯定是通过路由设备才能到达`Director`，这个路由器平时都将请求发送到主节点的`VIP`配置的`MAC`地址

上。要想和网卡的`VIP`进行通信最终还是依靠`MAC`地址。如果这个主节点故障了，那么就要想办法把这个请求发送到备节点的`MAC`

地址上。那么备节点的`MAC`地址和主节点的`MAC`地址肯定是不同的，而路由设备存放的`VIP`和`MAC`地址的对应关系还是原来路由器

和主节点的对应关系。`VIP`还是那个`VIP`，但是`VIP`和`MAC`的对应关系不同了。所以要想办法更新路由设备的`VIP`和`MAC`的对应关

系。当备用节点拿走这个`VIP`之后，一定要在第一时间让路由设备知道`VIP`和`MAC`的对应关系发生了改变。如何让路由设备得知这

个对应关系的改变。使用通告！通告不是随便进行通告的，而是这个主机第一次接入到网络的时候进行通告。平时，也就是在这个

之前备节点已经在这个网络中了，因为主备一直在心跳通信，所以已经不是第一次接入网略了，那么怎么让这个主机进行通告那 ?

就是**强行进行通告**。比如`VIP`已经到备节点了，那么我备节点就装模做样的进行一次广播，问`VIP`对应的`MAC`地址是多少啊，然后

备节点自问自答一下。因为是广播所以前面的路由设备也能收到广播信息。这个时候，路由设备发现自己缓存中的`VIP`和`MAC`信息

已经不一致了。那么这个时候就开始更新缓存。

```
HA Resource     HA资源
IP Service Stonith
Fail Over 故障转移
Fail Back 转移回去了（也不知道该咋翻译）
```

&emsp;资源倾向性，资源粘性。也就是资源更加倾向于在哪个节点上。汽车上有备胎，但是备胎是非全尺寸的备胎就是能用但是没有正

常的轮胎好。只是临时使用一次。主备节点也是类似的。主节点是非常好的功能强大的服务器，备节点性能差，只是临时用一下，

没有必要搞两个完全一样的节点，这样会浪费资源。总体应用成本浪费就会降低。所以这个时候资源粘性就出现了。服务优先于工

作于性能好的主节点上。

&emsp;两个节点怎么知道彼此的粘性，如何比较，高可用来提供。高可用通过传递集群事务信息，来传递粘性。

&emsp;**高可用事务**。事务信息的传递要通过专门的通道。集群事务信息的传递都是基于`UDP`的，因为集群事务信息的传递是强调效率

的，而不是强调可靠性的。

&emsp;**集群事务信息层**，两个节点之间专门用于传递集群事务信息的功能。主备节点都监听在`UDP`的某一个端口上。集群事务信息层，

从功能上来讲进行负责传递信息的。它不负责信息的处理和比较。那么谁来进行这个比较？专门负责集群事务信息，收集，并计

算，并提供管理功能的这部分叫做**资源管理器`CRM`**,也就是`cluster resource manager`。负责去统计和收集，集群资源状态，并

且根据资源状态和资源本身，计算出来资源应该运行在哪个节点上。然后才去相应的动作，让资源运行在对应的节点上。`CRM`的执

行结果决定了动作的执行，比如`CRM`计算出来了，应该`A`是主节点，那么无非就是通过`ifconfig`把`vip`和`ipvs`在主节点上工作起

来，然后再备节点停止下来。`CRM`就是一个程序，要运行在主备两个节点。是主节点上的负责计算还是备节点负责计算？`CRM`应该

运行在**集群事务协调员`DC`**上。谁是**集群事务协调员`DC`**，主节点和备节点协调出来的，推选的。如果`DC`所在的节点故障，那么要

重新推选出来一个节点，作为`DC`。`DC`是`CRM`的一个子功能，在`CRM`中。专门用来负责计算，粘性值，资源需要不需要转移，这个

功能就是运行在`DC`上的。比如现在`Active`节点是`DC`，那么`DC`就在`Active`节点上。`DC`必须提供两个功能，两个组件`PE`和`TE`。

也就是`policy engine`进行计算并得出结果的。`PE`是`CRM`的一个子功能。而`TE`也就是`transaction Engine`事务，**指挥**主备节

点进行资源转移。比如让备节点把资源`down`掉，并且把`IP`服务等进行转移，也就是把一个节点的`IP`和`LVS`全部转移到另外的一

个节点。`TE`指挥谁执行这个过程？`LRM`,它也是`crm`的一部分，运行在每一个节点上。**`LRM 本地资源管理器`**，负责接收`PE`发送来

的指挥，然后在每一个节点上。`LRM`停掉一个节点的`VIP`和`LVS`，然后再另外一个节点上开启`VIP`和`LVS`。`LRM`怎么停掉`VIP`使用

`ipconfig`等等方式停掉一个`VIP`。写一个脚本开启停止`LVS`，然后`LRM`调用这个脚本，完成切换。

&emsp;我们监控的时候，不但要监控节点，还要监控节点上的资源。比如我监控一个节点，这个节点可以`ping`通，但是这个节点上的

微服务挂了，那么我监控这个节点又有什么用处?所以我们要监控的是节点的资源。怎么监控资源是否正常？我们此前写的每一个

服务脚本都有一个`status`信息。比如`runing`。我要执行一下脚本看`status`信息。如果执行脚本发现服务宕机了，那么就再次检

测，多检测几次发现确实是宕机了，那么才开始转移。`crm`需要时刻检测服务的状态。脚本要输出资源的状态信息。脚本要接受四

个参数`start`,`stop`,`restart`,`status`。一个节点上的服务宕机了，那么先尝试把这个服务`restart`看看是否能拉起来，如果不

行再转移。因为重新拉起来要比转移，代价低。这就像我们项目中使用的`supervisor`一样。服务宕机首先使用`supervisor`拉起

来。如果拉不起来，那么继续尝试几次，如果还不行，那么开始主备倒换。不能上来就转移，因为转移的开销太大。

**&emsp;我们称接受`start`,`stop`,`restart`,`status`四个参数的脚本`LSB`的脚本**。`LSB`也就是`Linux stand Base`。我们的`VIP`也要提供

这个脚本。用于实现集群资源管理的脚本，能够用于提供集群资源的，启动，停止，重启，状态信息的脚本，我们称之为，`RA`

`RA`是`LRM`的上一层。每个资源都必须要有一个资源管理的脚本，我们称之为`RA`也就是`Resource Agent`，`RA`有很多，简称资源

代理。

​	![mg-高可用集群02]()

&emsp;注意上面的图形只是一个规范，那么提供`Messaging lay`的可以是不同的程序，提供`CRM`也可以是不同的程序。提供`RA`的也可

以是不通的程序。只要他们复合这个规范就可以了。

![mg-高可用集群03]()

&emsp;如果`ipvs`和`vip`在不通的节点上，如上图，那么怎么实现共同进退。所以又引入了另外的一个概念，资源组。

&emsp;**资源组**，`Resource Group`,把资源归为一组，那么就可以实现共进退了。

&emsp;还可以通过资源属性值，来定义资源之间的关系，我们期望`vip`和`ipvs`运行在一起。**高可用资源的资源约束Contraint**

资源和资源之间的倾向性是通过资源约束定义的。

资源和节点之间的倾向性是依靠资源粘性定义的。

&emsp;资源约束有三种：

（1）排列约束`colation constraint`

&emsp;用来定义资源之间的关系，这两个资源能否在一起，定义资源是否能够在同样的一个节点上。这个也是有`score分数`的。

正值就是可以在一起，负值表示不能在一起。

```
- inf 表示负无穷 两个绝对不能在同样的一个节点上
inf   表示正无穷   两个必须在一个节点上
```

（2）位置约束 	 `location constraint`

 `score分数`

正值：那么就是倾向于此

负值：倾向于逃离此。

什么是位置约束，比如有`A`节点和`P`节点，`A`节点的值是正值，`P`节点的值是负值，那么如果`A`节点故障了，那么现在`P`节点是负值。也要将就着运行在`P`节点上了，一旦有正值的节点出现了，那么就会执行迁移，逃离这个`P`节点。

（3）顺序约束	`order constraint`

&emsp;定义资源和资源之间的启动或者关闭顺序的。比如前面说的`VIP`和`IPVS`应该是谁先启动，谁后启动。你把`VIP`都配置上了，但是`IPVS`还没有配置上，那么别人的请求已经通过`VIP`过来了，怎么办？所以应该先配置`IPVS`然后再配置`VIP`。所以这个就是顺序约束。

&emsp;前面讲过的`stonith`，一点判断一个节点出现了故障，那么备节点就要把资源抢过来，但是怕主节点没有死，两个节点就会反

复的争夺。所以备节点就要爆头主节点，切断它的电源。这个叫做**资源隔离**。节点级别的隔离，资源级别的隔离。

资源级别：

&emsp;例如：`FC SAN switch`可以实现在存储资源级别拒绝某节点的访问。

&emsp;我们现在换一个集群，比如`mysql`集群。主备切换的时候需要使用共享存储设备。能够提供共享存储能力的设备，有：

`NFS`,`SCSI`。

&emsp;`VIP`也被称为`Float IP`。这种`mysql`集群，会使用共享存储，如果两个节点的`mysql`都向文件系统中写数据，那么就会出现问

题。所以当一个备节点抢占资源的时候一定要确保主节点挂了，防止脑裂（`split-brain`）的发生。脑裂的最严重的后果就是**抢**

**占共享存储**。出现脑裂的原因是双方无法进行有效的心跳信息的传递了。集群节点无法有效获取其他节点的状态信息时，产生脑

裂。后果之一：抢占共享存储。

&emsp;如果是主备这种模式的，那么就会有一个节点的资源被浪费了。所以我们想做一个两个`active`的集群。

​    可以出现两个节点都有相同的`VIP`，两个节点的`mysql`都在向文件系统中写入数据。前面说的一个节点工作，另外一个节点必须

死掉。但是后来发展了两个节点都可以运行的真正的两个节点`active`的集群。

&emsp;单机节点的文件系统最大的坏处就是一个节点在写的时候，其他的节点看不到。所以导致可能同时写。如果有一种机制让一个节

的写，别的节点也可以看到，这就是集群文件系统`Cluster filesystem`和`GFS`和`OCFS2`。如果要想使用集群文件系统，必须是

`DAS`或者`SAN`的设备，而不能是`NAS`。

&emsp;

&emsp;







- [参考文献1、DNS中的七大资源记录介绍]( https://blog.csdn.net/weixin_41545330/article/details/80865676 )
- [参考文献2、Rsync+Inotify实时同步环境部署记录](https://www.cnblogs.com/kevingrace/p/6001252.html)
- [参考文献3、lvs官网](http://www.linuxvirtualserver.org/zh/index.html)
- [参考文献4、网卡别名，很好看这一篇就能懂了](https://www.cnblogs.com/zengkefu/p/5475567.html)
- [参考文献5、VMware虚拟机三种网络模式（Centos虚拟机）](https://blog.csdn.net/zhang_xinxiu/article/details/84404848)
- 

